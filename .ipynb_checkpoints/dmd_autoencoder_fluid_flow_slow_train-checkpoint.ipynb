{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train fluid flow slow manifold Machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the nonlinear mean-field model of fluid flow past a circular cylinder at Reynolds number 100, described by empirical Galerkin model:\n",
    "$$\n",
    "\\dot{x_{1}} = \\mu x_{1} - \\omega x_{2} + A x_{1}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dot{x_{2}} = \\omega x_{1} + \\mu x_{2} + Ax_{2}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dot{x_{3}} = -\\lambda(x_{3} - {x_{1}}^{2} - {x_{2}}^{2}) \n",
    "$$\n",
    "\n",
    "Where $\\mu = 0.1, \\omega = 1, A=-0.1, \\lambda = 10$\n",
    "\n",
    "Let $\\mu = \\epsilon, \\omega = 1, A=-\\epsilon, \\lambda = \\epsilon^{-1}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{1}}{\\partial t} = \\epsilon x_{1} - x_{2} - \\epsilon x_{1}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{2}}{\\partial t} =  x_{1} + \\epsilon x_{2} - \\epsilon x_{2}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{3}}{\\partial t} = -\\epsilon^{-1}(x_{3} - {x_{1}}^{2} - {x_{2}}^{2}) \n",
    "$$\n",
    "\n",
    "\n",
    "Let $\\tilde{t} = \\epsilon t$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{1}}{\\partial \\tilde{t}} = \\epsilon^{2} x_{1} -  \\epsilon x_{2} - \\epsilon^{2} x_{1}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{2}}{\\partial \\tilde{t}} =   \\epsilon x_{1} + \\epsilon^{2} x_{2} - \\epsilon^{2} x_{2}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{3}}{\\partial \\tilde{t}} = - x_{3} + {x_{1}}^{2} + {x_{2}}^{2} \n",
    "$$\n",
    "\n",
    "\n",
    "Then \n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{1}}{\\partial \\tilde{t}}  \\approx  0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{2}}{\\partial \\tilde{t}} \\approx 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{3}}{\\partial \\tilde{t}} = - x_{3} + {x_{1}}^{2} + {x_{2}}^{2} \n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{3}(t) = \\tilde{c} e^{-t} + {x_{1}}^{2} + {x_{2}}^{2} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from dmd_machine.dmd_ae_machine import DMDMachine\n",
    "from dmd_machine.loss_function import LossFunction\n",
    "from data.Data import DataMaker\n",
    "from datetime import date \n",
    "from tensorflow.keras.models import model_from_json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from return_stats import *\n",
    "from create_plots import *\n",
    "from datetime import date  \n",
    "import pickle\n",
    "import time\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================================\n",
    "# Read in dataset.\n",
    "# ======================================================================================================================\n",
    "\n",
    "training_data = pickle.load(open('./data/dataset_fluid.pkl', 'rb'))\n",
    "\n",
    "input_data = training_data.data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Hyper Parameters.\n",
    "hyp_params = dict()\n",
    "hyp_params['num_t_steps'] = training_data.params['num_time_steps']\n",
    "hyp_params['phys_dim'] = training_data.params[\"num_physical_dim\"]\n",
    "hyp_params['num_init_conds'] = training_data.params['num_initial_conditions']\n",
    "hyp_params['batch_size'] = 256\n",
    "hyp_params['num_epochs'] = 200\n",
    "\n",
    "# Encoding/Decoding Layer Parameters.\n",
    "hyp_params['num_en_layers'] = 3\n",
    "hyp_params['num_en_neurons'] = 80\n",
    "hyp_params['latent_dim'] = 3\n",
    "hyp_params['window_size'] = 256\n",
    "\n",
    "hyp_params['activation'] = 'elu'\n",
    "hyp_params['weight_initializer'] = 'he_uniform'\n",
    "hyp_params['bias_initializer'] = 'he_uniform'\n",
    "hyp_params['ae_output_activation'] = \"linear\"\n",
    "hyp_params['hidden_activation'] = \"elu\"\n",
    "\n",
    "hyp_params['c1'] = 10  # coefficient auto-encoder loss.\n",
    "hyp_params['c2'] = 1  # coefficient of dmd loss.\n",
    "hyp_params['c3'] = 1  # coefficient of pred loss.\n",
    "\n",
    "# save results in the folder \" Results/save_folder\"- including loss curves and plot latent data.\n",
    "save_folder = \"AeEx3_\" + str(date.today().isoformat()) \n",
    "\n",
    "# number of initial conditions in training and testing dataset.\n",
    "hyp_params['num_init_conds_training'] = int(0.8 * hyp_params['num_init_conds'])\n",
    "hyp_params['num_init_conds_test'] = hyp_params['num_init_conds'] - hyp_params['num_init_conds_training']\n",
    "\n",
    "# initialize machine and loss objects.\n",
    "myMachine = DMDMachine(hyp_params)\n",
    "# myMachine.autoencoder = keras.models.load_model(\"./models/my_model_Ex2_oct21\", compile=False)\n",
    "myLoss = LossFunction(hyp_params)\n",
    "\n",
    "# Learning rate initialization.\n",
    "hyp_params[\"initial_learning_rate\"] = 3e-3  # MAJOR PARAMETER CHOICE\n",
    "hyp_params[\"esteps\"] = 60 # MAJOR PARAMETER CHOICE\n",
    "count = 0\n",
    "\n",
    "# clear previous run session.\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# create folder to save results.\n",
    "create_new_folders(save_folder)\n",
    "\n",
    "# save hyperparams in a json file.\n",
    "save_hyp_params_in_json(hyp_params=hyp_params, json_file_path=os.path.join(\"results\", save_folder, \"hyp_params.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of training dataset (ic x phys_dim x timesteps) =  (8000, 3, 121)\n",
      "dimensions of testing dataset (ic x phys_dim x timesteps) =  (2000, 3, 121)\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================================================================\n",
    "# Prepare dataset. \n",
    "# ======================================================================================================================\n",
    "# shuffle the dataset and then divide to training vs testing data sets. 80% training .20% testing.\n",
    "data_train, data_test= train_test_split(input_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"dimensions of training dataset (ic x phys_dim x timesteps) = \", np.shape(data_train))\n",
    "print(\"dimensions of testing dataset (ic x phys_dim x timesteps) = \", np.shape(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================================\n",
    "# Unit test to verify that testing and training datasets are disjoint.\n",
    "# ======================================================================================================================\n",
    "for ic_train in data_train:\n",
    "    for ic_test in data_test:\n",
    "        if ic_test[:, 0][0] == ic_train[:, 0][0] and ic_test[:, 0][1] == ic_train[:, 0][1]\\\n",
    "        and ic_test[:, 0][2] == ic_train[:, 0][2]:\n",
    "            print(\"Testing and Training datasets intersect!\")\n",
    "            print(ic_test[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datasets from numpy to tensorflow.\n",
    "data_train =  tf.data.Dataset.from_tensor_slices(data_train)\n",
    "data_test =  tf.data.Dataset.from_tensor_slices(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/200 ,loss_train: 5.821043 , loss_test: 0.5425728, run time = 719.828125 sec.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000018849E248B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000018849E248B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001885E981E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001885E981E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001883B5570D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001883B5570D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001886E096AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001886E096AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001886E096828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001886E096828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\encAeEx3_2020-12-04\\assets\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000018849E24828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000018849E24828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001886E096558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001886E096558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001886E096318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001886E096318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001883B557048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001883B557048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001886E0963A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x000001886E0963A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\decAeEx3_2020-12-04\\assets\n",
      "10/200 ,loss_train: 0.007525408 , loss_test: 0.007283557, run time = 704.515625 sec.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================================================================\n",
    "# Begin training model\n",
    "# ======================================================================================================================\n",
    "\n",
    "# initialize loss results (lists) as a function of epoch (iteration).\n",
    "train_loss_results = []\n",
    "test_loss_results = []\n",
    "\n",
    "train_dmd_loss = []\n",
    "test_dmd_loss = []\n",
    "\n",
    "train_ae_loss = []\n",
    "test_ae_loss = []\n",
    "\n",
    "train_pred_loss = []\n",
    "test_pred_loss = []\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "while epoch < (hyp_params['num_epochs']):\n",
    "    # start timer.\n",
    "    start_time = time.process_time()\n",
    "    # save the total loss of the training data and testing data.\n",
    "    epoch_loss_avg_train = tf.keras.metrics.Mean()\n",
    "    epoch_loss_avg_test = tf.keras.metrics.Mean()\n",
    "\n",
    "    # keep track of individual losses as well, aka dmd loss and ae loss.\n",
    "    epoch_loss_dmd_train = tf.keras.metrics.Mean()\n",
    "    epoch_loss_dmd_test = tf.keras.metrics.Mean()\n",
    "\n",
    "    epoch_loss_ae_train = tf.keras.metrics.Mean()\n",
    "    epoch_loss_ae_test = tf.keras.metrics.Mean()\n",
    "\n",
    "    epoch_loss_pred_train = tf.keras.metrics.Mean()\n",
    "    epoch_loss_pred_test = tf.keras.metrics.Mean()\n",
    "\n",
    "    # Build out the batches within a given epoch.\n",
    "    train_batch = data_train.shuffle(hyp_params['num_init_conds_training'], seed=42).batch(hyp_params[\"batch_size\"],\n",
    "                                                                                           drop_remainder=True)\n",
    "\n",
    "    # no need to shuffle test dataset.\n",
    "    test_batch = data_test.batch(hyp_params[\"batch_size\"], drop_remainder=True)\n",
    "\n",
    "    # Learning rate scheduling plan.  See Ch. 11 of O'Reilly.\n",
    "    if epoch % hyp_params[\"esteps\"] == 0:\n",
    "        hyp_params['lr'] = (.1 ** count) * hyp_params[\"initial_learning_rate\"]\n",
    "        adam_optimizer = tf.keras.optimizers.Adam(hyp_params['lr'])\n",
    "        count += 1\n",
    "\n",
    "    # Iterate through all the batches within an epoch.\n",
    "    for batch_training_data in train_batch:\n",
    "        # normalize batch\n",
    "\n",
    "        # Build terms that we differentiate (i.e. loss) and that we differentiate with respect to.\n",
    "        with tf.GradientTape() as tape:\n",
    "            # training=True is only needed if there are layers with different\n",
    "            # behavior during training versus inference (e.g. Dropout).\n",
    "            predictions_train = myMachine(batch_training_data)\n",
    "            ae_loss = predictions_train[3]\n",
    "            dmd_loss = predictions_train[2]\n",
    "            pred_loss = predictions_train[5]\n",
    "\n",
    "            loss_train = myLoss(batch_training_data, predictions_train)\n",
    "\n",
    "        # Compute gradients and then apply them to update weights within the Neural Network\n",
    "        gradients = tape.gradient(loss_train, myMachine.trainable_variables)\n",
    "        adam_optimizer.apply_gradients([\n",
    "            (grad, var)\n",
    "            for (grad, var) in zip(gradients, myMachine.trainable_variables)\n",
    "            if grad is not None\n",
    "        ])\n",
    "\n",
    "        # Keep track of the loss after each batch.\n",
    "        epoch_loss_avg_train.update_state(loss_train)\n",
    "        epoch_loss_ae_train.update_state(ae_loss)\n",
    "        epoch_loss_dmd_train.update_state(dmd_loss)\n",
    "        epoch_loss_pred_train.update_state(pred_loss)\n",
    "\n",
    "    for batch_test_data in test_batch:\n",
    "        predictions_test = myMachine(batch_test_data)\n",
    "        dmd_test = predictions_test[2]\n",
    "        ae_test = predictions_test[3]\n",
    "        pred_test = predictions_test[5]\n",
    "\n",
    "        loss_test = myLoss(batch_test_data, predictions_test)\n",
    "\n",
    "        epoch_loss_avg_test.update_state(loss_test)\n",
    "        epoch_loss_ae_test.update_state(ae_test)\n",
    "        epoch_loss_dmd_test.update_state(dmd_test)\n",
    "        epoch_loss_pred_test.update_state(pred_test)\n",
    "\n",
    "    train_loss_results.append(epoch_loss_avg_train.result())\n",
    "    test_loss_results.append(epoch_loss_avg_test.result())\n",
    "\n",
    "    train_dmd_loss.append(epoch_loss_dmd_train.result())\n",
    "    train_ae_loss.append(epoch_loss_ae_train.result())\n",
    "    train_pred_loss.append(epoch_loss_pred_train.result())\n",
    "\n",
    "    test_dmd_loss.append(epoch_loss_dmd_test.result())\n",
    "    test_ae_loss.append(epoch_loss_ae_test.result())\n",
    "    test_pred_loss.append(epoch_loss_pred_test.result())\n",
    "\n",
    "    if epoch % 15 == 0:\n",
    "        # save plots in results folder. Plot the latent space, ae_reconstruction, and input_batch.\n",
    "        create_plots_fluid_pred(batch_training_data, predictions_train, hyp_params, epoch, save_folder, \"train\")\n",
    "        create_plots_fluid_pred(batch_test_data, predictions_test, hyp_params, epoch, save_folder, \"test\")\n",
    "\n",
    "        # fluid latent space plots.\n",
    "        create_plots_fluid_latent(predictions_train, hyp_params, epoch,  save_folder, data_type=\"train\")\n",
    "        create_plots_fluid_latent(predictions_test, hyp_params, epoch, save_folder, data_type=\"test\")\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        # plot latent, input and reconstructed ae latest batch data.\n",
    "        print_status_bar(epoch, hyp_params[\"num_epochs\"], epoch_loss_avg_train.result(),\n",
    "                         epoch_loss_avg_test.result(), time.process_time() - start_time,\n",
    "                         log_file_path=os.path.join(\"results\", save_folder, \"log.txt\"))\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        # plot loss curves.\n",
    "        create_plots_of_loss(train_dmd_loss, train_ae_loss, test_dmd_loss, test_ae_loss, train_pred_loss,\n",
    "                             test_pred_loss, myLoss.c1, myLoss.c2, myLoss.c3, epoch, save_folder)\n",
    "\n",
    "        # save loss curves in pickle files.\n",
    "        save_loss_curves(train_loss_results, test_loss_results, train_dmd_loss, test_dmd_loss, train_ae_loss,\n",
    "                         test_ae_loss, train_pred_loss, test_pred_loss,\n",
    "                         file_path=os.path.join(\"results\", save_folder, \"Loss\"))\n",
    "\n",
    "        # save current machine.\n",
    "        myMachine.autoencoder.encoder.save(os.path.join(\"models\", str(\"enc\") + save_folder), save_format='save_weights')\n",
    "        myMachine.autoencoder.decoder.save(os.path.join(\"models\", str(\"dec\") + save_folder), save_format='save_weights')\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "# final summary of the network, again for diagnostic purposes.\n",
    "myMachine.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
