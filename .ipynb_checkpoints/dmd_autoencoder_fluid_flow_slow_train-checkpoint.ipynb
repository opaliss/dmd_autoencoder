{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train fluid flow slow manifold Machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the nonlinear mean-field model of fluid flow past a circular cylinder at Reynolds number 100, described by empirical Galerkin model:\n",
    "$$\n",
    "\\dot{x_{1}} = \\mu x_{1} - \\omega x_{2} + A x_{1}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dot{x_{2}} = \\omega x_{1} + \\mu x_{2} + Ax_{2}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dot{x_{3}} = -\\lambda(x_{3} - {x_{1}}^{2} - {x_{2}}^{2}) \n",
    "$$\n",
    "\n",
    "Where $\\mu = 0.1, \\omega = 1, A=-0.1, \\lambda = 10$\n",
    "\n",
    "Let $\\mu = \\epsilon, \\omega = 1, A=-\\epsilon, \\lambda = \\epsilon^{-1}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{1}}{\\partial t} = \\epsilon x_{1} - x_{2} - \\epsilon x_{1}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{2}}{\\partial t} =  x_{1} + \\epsilon x_{2} - \\epsilon x_{2}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{3}}{\\partial t} = -\\epsilon^{-1}(x_{3} - {x_{1}}^{2} - {x_{2}}^{2}) \n",
    "$$\n",
    "\n",
    "\n",
    "Let $\\tilde{t} = t/\\epsilon $\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{1}}{\\partial \\tilde{t}} = \\epsilon^{2} x_{1} -  \\epsilon x_{2} - \\epsilon^{2} x_{1}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{2}}{\\partial \\tilde{t}} =   \\epsilon x_{1} + \\epsilon^{2} x_{2} - \\epsilon^{2} x_{2}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{3}}{\\partial \\tilde{t}} = - x_{3} + {x_{1}}^{2} + {x_{2}}^{2} \n",
    "$$\n",
    "\n",
    "\n",
    "Then \n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{1}}{\\partial \\tilde{t}}  \\approx  0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{2}}{\\partial \\tilde{t}} \\approx 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{3}}{\\partial \\tilde{t}} = - x_{3} + {x_{1}}^{2} + {x_{2}}^{2} \n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{3}(t) = \\tilde{c} e^{-t} + {x_{1}}^{2} + {x_{2}}^{2} \n",
    "$$\n",
    "\n",
    "if \n",
    "\n",
    "$$\n",
    "x_{1}(0) = r \\cos(\\theta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{2}(0) = r \\sin(\\theta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{3}(0) = \\tilde{c} e^{-0} + {r \\cos(\\theta)}^{2} + {r \\sin(\\theta)}^{2}= \\tilde{c} + r^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from dmd_machine.dmd_ae_machine import DMDMachine\n",
    "from dmd_machine.loss_function import LossFunction\n",
    "from data.Data import DataMaker\n",
    "from datetime import date \n",
    "from tensorflow.keras.models import model_from_json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from return_stats import *\n",
    "from create_plots import *\n",
    "from tensorflow import keras\n",
    "from datetime import date  \n",
    "import pickle\n",
    "import time\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================================\n",
    "# Read in dataset.\n",
    "# ======================================================================================================================\n",
    "\n",
    "training_data = pickle.load(open('./data/dataset_fluid.pkl', 'rb'))\n",
    "\n",
    "input_data = training_data.data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Hyper Parameters.\n",
    "hyp_params = dict()\n",
    "hyp_params['num_t_steps'] = training_data.params['num_time_steps']\n",
    "hyp_params['phys_dim'] = training_data.params[\"num_physical_dim\"]\n",
    "hyp_params['num_init_conds'] = training_data.params['num_initial_conditions']\n",
    "hyp_params['batch_size'] = 256\n",
    "hyp_params['num_epochs'] = 500\n",
    "\n",
    "# Encoding/Decoding Layer Parameters.\n",
    "hyp_params['num_en_layers'] = 3\n",
    "hyp_params['num_en_neurons'] = 80\n",
    "hyp_params['latent_dim'] = 2 # NEW EXPERIMENT!\n",
    "hyp_params['window_size'] = 256\n",
    "\n",
    "hyp_params['activation'] = 'elu'\n",
    "hyp_params['weight_initializer'] = 'he_uniform'\n",
    "hyp_params['bias_initializer'] = 'he_uniform'\n",
    "hyp_params['ae_output_activation'] = \"linear\"\n",
    "hyp_params['hidden_activation'] = \"elu\"\n",
    "\n",
    "hyp_params['c1'] = 1  # coefficient auto-encoder loss.\n",
    "hyp_params['c2'] = 1  # coefficient of dmd loss.\n",
    "hyp_params['c3'] = 1  # coefficient of pred loss.\n",
    "\n",
    "# save results in the folder \" Results/save_folder\"- including loss curves and plot latent data.\n",
    "save_folder = \"AeEx3_\" + str(date.today().isoformat()) \n",
    "\n",
    "# number of initial conditions in training and testing dataset.\n",
    "hyp_params['num_init_conds_training'] = int(0.8 * hyp_params['num_init_conds'])\n",
    "hyp_params['num_init_conds_test'] = hyp_params['num_init_conds'] - hyp_params['num_init_conds_training']\n",
    "\n",
    "# initialize machine and loss objects.\n",
    "myMachine = DMDMachine(hyp_params)\n",
    "# myMachine.autoencoder = keras.models.load_model(\"./models/my_model_Ex2_oct21\", compile=False)\n",
    "myLoss = LossFunction(hyp_params)\n",
    "\n",
    "# Learning rate initialization.\n",
    "hyp_params[\"initial_learning_rate\"] = 3e-3  # MAJOR PARAMETER CHOICE\n",
    "hyp_params[\"esteps\"] = 40 # MAJOR PARAMETER CHOICE\n",
    "count = 0\n",
    "\n",
    "# clear previous run session.\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# create folder to save results.\n",
    "create_new_folders(save_folder)\n",
    "\n",
    "# save hyperparams in a json file.\n",
    "save_hyp_params_in_json(hyp_params=hyp_params, json_file_path=os.path.join(\"results\", save_folder, \"hyp_params.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================================\n",
    "# Load previously trained model as the intitial weights + biases. \n",
    "# ======================================================================================================================\n",
    "load_prev = True \n",
    "\n",
    "if load_prev: \n",
    "    myMachine.autoencoder.encoder = keras.models.load_model(\"./models/encAeEx3_2020-12-13\", compile=False)\n",
    "    myMachine.autoencoder.decoder = keras.models.load_model(\"./models/decAeEx3_2020-12-13\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of training dataset (ic x phys_dim x timesteps) =  (8000, 3, 121)\n",
      "dimensions of testing dataset (ic x phys_dim x timesteps) =  (2000, 3, 121)\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================================================================\n",
    "# Prepare dataset. \n",
    "# ======================================================================================================================\n",
    "# shuffle the dataset and then divide to training vs testing data sets. 80% training .20% testing.\n",
    "data_train, data_test= train_test_split(input_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"dimensions of training dataset (ic x phys_dim x timesteps) = \", np.shape(data_train))\n",
    "print(\"dimensions of testing dataset (ic x phys_dim x timesteps) = \", np.shape(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================================\n",
    "# Unit test to verify that testing and training datasets are disjoint.\n",
    "# ======================================================================================================================\n",
    "for ic_train in data_train:\n",
    "    for ic_test in data_test:\n",
    "        if ic_test[:, 0][0] == ic_train[:, 0][0] and ic_test[:, 0][1] == ic_train[:, 0][1]\\\n",
    "        and ic_test[:, 0][2] == ic_train[:, 0][2]:\n",
    "            print(\"Testing and Training datasets intersect!\")\n",
    "            print(ic_test[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datasets from numpy to tensorflow.\n",
    "data_train =  tf.data.Dataset.from_tensor_slices(data_train)\n",
    "data_test =  tf.data.Dataset.from_tensor_slices(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/500 ,loss_train: 0.314832 , loss_test: 0.04656554, run time = 455.3125 sec.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020A86A0C3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020A86A0C3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000020B1A1F2B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000020B1A1F2B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000020B1A1F2EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000020B1A1F2EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000020B1A13D288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000020B1A13D288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000020B1A13D5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000020B1A13D5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\encAeEx3_2020-12-14\\assets\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020A86A9EF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020A86A9EF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000020A9A48C288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000020A9A48C288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000020A9A48C5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000020A9A48C5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000020A9A48C948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000020A9A48C948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000020A9A48CCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000020A9A48CCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\decAeEx3_2020-12-14\\assets\n",
      "10/500 ,loss_train: 0.00023115944 , loss_test: 0.00022008303, run time = 349.875 sec.\n",
      "20/500 ,loss_train: 0.00011124912 , loss_test: 0.00012681803, run time = 350.03125 sec.\n",
      "30/500 ,loss_train: 6.694348e-05 , loss_test: 7.215147e-05, run time = 607.265625 sec.\n",
      "40/500 ,loss_train: 0.0041608415 , loss_test: 0.00014053569, run time = 261.15625 sec.\n",
      "50/500 ,loss_train: 4.08301e-05 , loss_test: 4.1098858e-05, run time = 140.890625 sec.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020AADA95828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020AADA95828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\encAeEx3_2020-12-14\\assets\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020AADAEAAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020AADAEAAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\decAeEx3_2020-12-14\\assets\n",
      "60/500 ,loss_train: 3.846105e-05 , loss_test: 5.4656204e-05, run time = 162.484375 sec.\n",
      "70/500 ,loss_train: 2.8869938e-05 , loss_test: 3.3953806e-05, run time = 134.84375 sec.\n",
      "80/500 ,loss_train: 4.0064155e-05 , loss_test: 2.5980427e-05, run time = 141.171875 sec.\n",
      "90/500 ,loss_train: 1.9605017e-05 , loss_test: 2.0181165e-05, run time = 160.03125 sec.\n",
      "100/500 ,loss_train: 1.8002944e-05 , loss_test: 1.8435618e-05, run time = 133.5 sec.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020AC456C438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020AC456C438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\encAeEx3_2020-12-14\\assets\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020AC55DF708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020AC55DF708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\decAeEx3_2020-12-14\\assets\n",
      "110/500 ,loss_train: 1.661835e-05 , loss_test: 1.7156917e-05, run time = 133.390625 sec.\n",
      "120/500 ,loss_train: 1.518461e-05 , loss_test: 1.551378e-05, run time = 153.03125 sec.\n",
      "130/500 ,loss_train: 1.4718998e-05 , loss_test: 1.5213761e-05, run time = 138.765625 sec.\n",
      "140/500 ,loss_train: 1.4359841e-05 , loss_test: 1.4842041e-05, run time = 138.671875 sec.\n",
      "150/500 ,loss_train: 1.4016793e-05 , loss_test: 1.4488135e-05, run time = 157.34375 sec.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020B04CE03A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020B04CE03A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\encAeEx3_2020-12-14\\assets\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020B04CE0F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020B04CE0F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\decAeEx3_2020-12-14\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/500 ,loss_train: 1.3678649e-05 , loss_test: 1.4094281e-05, run time = 336.1875 sec.\n",
      "170/500 ,loss_train: 1.36184835e-05 , loss_test: 1.4034943e-05, run time = 345.703125 sec.\n",
      "180/500 ,loss_train: 1.3561095e-05 , loss_test: 1.3976921e-05, run time = 388.09375 sec.\n",
      "190/500 ,loss_train: 1.3503742e-05 , loss_test: 1.3917695e-05, run time = 330.0 sec.\n",
      "200/500 ,loss_train: 1.344413e-05 , loss_test: 1.3849577e-05, run time = 136.921875 sec.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020B1A506288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020B1A506288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\encAeEx3_2020-12-14\\assets\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020B1A506EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020B1A506EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\decAeEx3_2020-12-14\\assets\n",
      "210/500 ,loss_train: 1.3435828e-05 , loss_test: 1.38431915e-05, run time = 158.328125 sec.\n",
      "220/500 ,loss_train: 1.3428849e-05 , loss_test: 1.3835911e-05, run time = 137.859375 sec.\n",
      "230/500 ,loss_train: 1.3421696e-05 , loss_test: 1.3828508e-05, run time = 141.53125 sec.\n",
      "240/500 ,loss_train: 1.3414335e-05 , loss_test: 1.3821697e-05, run time = 160.203125 sec.\n",
      "250/500 ,loss_train: 1.3414214e-05 , loss_test: 1.3821635e-05, run time = 147.3125 sec.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020B3D877678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020B3D877678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\encAeEx3_2020-12-14\\assets\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020B3D877948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020B3D877948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\decAeEx3_2020-12-14\\assets\n",
      "260/500 ,loss_train: 1.34141055e-05 , loss_test: 1.38215755e-05, run time = 140.546875 sec.\n",
      "270/500 ,loss_train: 1.3414061e-05 , loss_test: 1.382138e-05, run time = 151.28125 sec.\n",
      "280/500 ,loss_train: 1.3413967e-05 , loss_test: 1.3821438e-05, run time = 148.375 sec.\n",
      "290/500 ,loss_train: 1.3413939e-05 , loss_test: 1.3821409e-05, run time = 159.9375 sec.\n",
      "300/500 ,loss_train: 1.3413956e-05 , loss_test: 1.3821336e-05, run time = 158.3125 sec.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020B428453A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020B428453A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\encAeEx3_2020-12-14\\assets\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020B428D4F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000020B428D4F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\decAeEx3_2020-12-14\\assets\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b9b87f568327>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[1;31m# training=True is only needed if there are layers with different\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;31m# behavior during training versus inference (e.g. Dropout).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mpredictions_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyMachine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_training_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m             \u001b[0mae_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdmd_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    967\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 968\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\dmd_autoencoder\\dmd_machine\\dmd_ae_machine.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_predict_batch_reshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_data_mat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# decode predicted latent space.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\dmd_autoencoder\\dmd_machine\\dmd_ae_machine.py\u001b[0m in \u001b[0;36mcompute_predict_batch_reshape\u001b[1;34m(self, y_data_mat)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;31m# find the big A mat (512 x 512)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mpred_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_predicted_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_reshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;31m# undo reshape- back to batch_size, phys_dim, t_steps.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\dmd_autoencoder\\dmd_machine\\dmd_ae_machine.py\u001b[0m in \u001b[0;36mget_predicted_y\u001b[1;34m(self, y_data)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_t_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[0mA_exp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_exp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m             \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mii\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_exp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   2982\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2983\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[1;32m-> 2984\u001b[1;33m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[0;32m   2985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   5565\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5566\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_b\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5567\u001b[1;33m         transpose_b)\n\u001b[0m\u001b[0;32m   5568\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5569\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ======================================================================================================================\n",
    "# Begin training model\n",
    "# ======================================================================================================================\n",
    "\n",
    "# initialize loss results (lists) as a function of epoch (iteration).\n",
    "train_loss_results = []\n",
    "test_loss_results = []\n",
    "\n",
    "train_dmd_loss = []\n",
    "test_dmd_loss = []\n",
    "\n",
    "train_ae_loss = []\n",
    "test_ae_loss = []\n",
    "\n",
    "train_pred_loss = []\n",
    "test_pred_loss = []\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "while epoch < (hyp_params['num_epochs']):\n",
    "    # start timer.\n",
    "    start_time = time.process_time()\n",
    "    # save the total loss of the training data and testing data.\n",
    "    epoch_loss_avg_train = tf.keras.metrics.Mean()\n",
    "    epoch_loss_avg_test = tf.keras.metrics.Mean()\n",
    "\n",
    "    # keep track of individual losses as well, aka dmd loss and ae loss.\n",
    "    epoch_loss_dmd_train = tf.keras.metrics.Mean()\n",
    "    epoch_loss_dmd_test = tf.keras.metrics.Mean()\n",
    "\n",
    "    epoch_loss_ae_train = tf.keras.metrics.Mean()\n",
    "    epoch_loss_ae_test = tf.keras.metrics.Mean()\n",
    "\n",
    "    epoch_loss_pred_train = tf.keras.metrics.Mean()\n",
    "    epoch_loss_pred_test = tf.keras.metrics.Mean()\n",
    "\n",
    "    # Build out the batches within a given epoch.\n",
    "    train_batch = data_train.shuffle(hyp_params['num_init_conds_training'], seed=42).batch(hyp_params[\"batch_size\"],\n",
    "                                                                                           drop_remainder=True)\n",
    "\n",
    "    # no need to shuffle test dataset.\n",
    "    test_batch = data_test.batch(hyp_params[\"batch_size\"], drop_remainder=True)\n",
    "\n",
    "    # Learning rate scheduling plan.  See Ch. 11 of O'Reilly.\n",
    "    if epoch % hyp_params[\"esteps\"] == 0:\n",
    "        hyp_params['lr'] = (.1 ** count) * hyp_params[\"initial_learning_rate\"]\n",
    "        adam_optimizer = tf.keras.optimizers.Adam(hyp_params['lr'])\n",
    "        count += 1\n",
    "\n",
    "    # Iterate through all the batches within an epoch.\n",
    "    for batch_training_data in train_batch:\n",
    "        # normalize batch\n",
    "\n",
    "        # Build terms that we differentiate (i.e. loss) and that we differentiate with respect to.\n",
    "        with tf.GradientTape() as tape:\n",
    "            # training=True is only needed if there are layers with different\n",
    "            # behavior during training versus inference (e.g. Dropout).\n",
    "            predictions_train = myMachine(batch_training_data)\n",
    "            ae_loss = predictions_train[3]\n",
    "            dmd_loss = predictions_train[2]\n",
    "            pred_loss = predictions_train[5]\n",
    "\n",
    "            loss_train = myLoss(batch_training_data, predictions_train)\n",
    "\n",
    "        # Compute gradients and then apply them to update weights within the Neural Network\n",
    "        gradients = tape.gradient(loss_train, myMachine.trainable_variables)\n",
    "        adam_optimizer.apply_gradients([\n",
    "            (grad, var)\n",
    "            for (grad, var) in zip(gradients, myMachine.trainable_variables)\n",
    "            if grad is not None\n",
    "        ])\n",
    "\n",
    "        # Keep track of the loss after each batch.\n",
    "        epoch_loss_avg_train.update_state(loss_train)\n",
    "        epoch_loss_ae_train.update_state(ae_loss)\n",
    "        epoch_loss_dmd_train.update_state(dmd_loss)\n",
    "        epoch_loss_pred_train.update_state(pred_loss)\n",
    "\n",
    "    for batch_test_data in test_batch:\n",
    "        predictions_test = myMachine(batch_test_data)\n",
    "        dmd_test = predictions_test[2]\n",
    "        ae_test = predictions_test[3]\n",
    "        pred_test = predictions_test[5]\n",
    "\n",
    "        loss_test = myLoss(batch_test_data, predictions_test)\n",
    "\n",
    "        epoch_loss_avg_test.update_state(loss_test)\n",
    "        epoch_loss_ae_test.update_state(ae_test)\n",
    "        epoch_loss_dmd_test.update_state(dmd_test)\n",
    "        epoch_loss_pred_test.update_state(pred_test)\n",
    "\n",
    "    train_loss_results.append(epoch_loss_avg_train.result())\n",
    "    test_loss_results.append(epoch_loss_avg_test.result())\n",
    "\n",
    "    train_dmd_loss.append(epoch_loss_dmd_train.result())\n",
    "    train_ae_loss.append(epoch_loss_ae_train.result())\n",
    "    train_pred_loss.append(epoch_loss_pred_train.result())\n",
    "\n",
    "    test_dmd_loss.append(epoch_loss_dmd_test.result())\n",
    "    test_ae_loss.append(epoch_loss_ae_test.result())\n",
    "    test_pred_loss.append(epoch_loss_pred_test.result())\n",
    "\n",
    "    if epoch % 15 == 0:\n",
    "        # save plots in results folder. Plot the latent space, ae_reconstruction, and input_batch.\n",
    "        create_plots_fluid_pred(batch_training_data, predictions_train, hyp_params, epoch, save_folder, \"train\")\n",
    "        create_plots_fluid_pred(batch_test_data, predictions_test, hyp_params, epoch, save_folder, \"test\")\n",
    "        \n",
    "        # if latent dim is 3 dimensional then create 3d plots. \n",
    "        if hyp_params[\"latent_dim\"] == 3: \n",
    "            # fluid latent space plots.\n",
    "            create_plots_fluid_latent_3d(predictions_train, hyp_params, epoch,  save_folder, data_type=\"train\")\n",
    "            create_plots_fluid_latent_3d(predictions_test, hyp_params, epoch, save_folder, data_type=\"test\")\n",
    "        \n",
    "        # if latent fim is 2 dimensional, then create 2d plots. \n",
    "        elif hyp_params[\"latent_dim\"] == 2: \n",
    "            create_plots_fluid_latent_2d(predictions_train, hyp_params, epoch,  save_folder, data_type=\"train\")\n",
    "            create_plots_fluid_latent_2d(predictions_test, hyp_params, epoch, save_folder, data_type=\"test\")\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        # plot latent, input and reconstructed ae latest batch data.\n",
    "        print_status_bar(epoch, hyp_params[\"num_epochs\"], epoch_loss_avg_train.result(),\n",
    "                         epoch_loss_avg_test.result(), time.process_time() - start_time,\n",
    "                         log_file_path=os.path.join(\"results\", save_folder, \"log.txt\"))\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        # plot loss curves.\n",
    "        create_plots_of_loss(train_dmd_loss, train_ae_loss, test_dmd_loss, test_ae_loss, train_pred_loss,\n",
    "                             test_pred_loss, myLoss.c1, myLoss.c2, myLoss.c3, epoch, save_folder)\n",
    "\n",
    "        # save loss curves in pickle files.\n",
    "        save_loss_curves(train_loss_results, test_loss_results, train_dmd_loss, test_dmd_loss, train_ae_loss,\n",
    "                         test_ae_loss, train_pred_loss, test_pred_loss,\n",
    "                         file_path=os.path.join(\"results\", save_folder, \"Loss\"))\n",
    "\n",
    "        # save current machine.\n",
    "        myMachine.autoencoder.encoder.save(os.path.join(\"models\", str(\"enc\") + save_folder), save_format='save_weights')\n",
    "        myMachine.autoencoder.decoder.save(os.path.join(\"models\", str(\"dec\") + save_folder), save_format='save_weights')\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "# final summary of the network, again for diagnostic purposes.\n",
    "myMachine.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
