{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Fluid Flow slow manifold Machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the nonlinear mean-field model of fluid flow past a circular cylinder at Reynolds number 100, described by empirical Galerkin model:\n",
    "$$\n",
    "\\dot{x_{1}} = \\mu x_{1} - \\omega x_{2} + A x_{1}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dot{x_{2}} = \\omega x_{1} + \\mu x_{2} + Ax_{2}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dot{x_{3}} = -\\lambda(x_{3} - {x_{1}}^{2} - {x_{2}}^{2}) \n",
    "$$\n",
    "\n",
    "Where $\\mu = 0.1, \\omega = 1, A=-0.1, \\lambda = 10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from dmd_machine.dmd_ae_machine import DMDMachine\n",
    "from dmd_machine.loss_function import LossFunction\n",
    "from data.Data import DataMaker\n",
    "from datetime import date \n",
    "from tensorflow.keras.models import model_from_json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from return_stats import *\n",
    "from create_plots import *\n",
    "from datetime import date  \n",
    "import pickle\n",
    "import time\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================================\n",
    "# Read in dataset.\n",
    "# ======================================================================================================================\n",
    "\n",
    "training_data = pickle.load(open('./data/dataset_fluid.pkl', 'rb'))\n",
    "\n",
    "input_data = training_data.data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Hyper Parameters.\n",
    "hyp_params = dict()\n",
    "hyp_params['num_t_steps'] = training_data.params['num_time_steps']\n",
    "hyp_params['phys_dim'] = training_data.params[\"num_physical_dim\"]\n",
    "hyp_params['num_init_conds'] = training_data.params['num_initial_conditions']\n",
    "hyp_params['batch_size'] = 256\n",
    "hyp_params['num_epochs'] = 200\n",
    "\n",
    "# Encoding/Decoding Layer Parameters.\n",
    "hyp_params['num_en_layers'] = 3\n",
    "hyp_params['num_en_neurons'] = 80\n",
    "hyp_params['latent_dim'] = 3\n",
    "hyp_params['window_size'] = 256\n",
    "\n",
    "hyp_params['activation'] = 'elu'\n",
    "hyp_params['weight_initializer'] = 'he_uniform'\n",
    "hyp_params['bias_initializer'] = 'he_uniform'\n",
    "hyp_params['ae_output_activation'] = \"linear\"\n",
    "hyp_params['hidden_activation'] = \"elu\"\n",
    "\n",
    "hyp_params['c1'] = 10  # coefficient auto-encoder loss.\n",
    "hyp_params['c2'] = 1  # coefficient of dmd loss.\n",
    "hyp_params['c3'] = 1  # coefficient of pred loss.\n",
    "\n",
    "# save results in the folder \" Results/save_folder\"- including loss curves and plot latent data.\n",
    "save_folder = \"AeEx3_\" + str(date.today().isoformat()) \n",
    "\n",
    "# number of initial conditions in training and testing dataset.\n",
    "hyp_params['num_init_conds_training'] = int(0.8 * hyp_params['num_init_conds'])\n",
    "hyp_params['num_init_conds_test'] = hyp_params['num_init_conds'] - hyp_params['num_init_conds_training']\n",
    "\n",
    "# initialize machine and loss objects.\n",
    "myMachine = DMDMachine(hyp_params)\n",
    "# myMachine.autoencoder = keras.models.load_model(\"./models/my_model_Ex2_oct21\", compile=False)\n",
    "myLoss = LossFunction(hyp_params)\n",
    "\n",
    "# Learning rate initialization.\n",
    "hyp_params[\"initial_learning_rate\"] = 3e-4  # MAJOR PARAMETER CHOICE\n",
    "hyp_params[\"esteps\"] = 30  # MAJOR PARAMETER CHOICE\n",
    "count = 0\n",
    "\n",
    "# clear previous run session.\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# create folder to save results.\n",
    "create_new_folders(save_folder)\n",
    "\n",
    "# save hyperparams in a json file.\n",
    "save_hyp_params_in_json(hyp_params=hyp_params, json_file_path=os.path.join(\"results\", save_folder, \"hyp_params.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of training dataset (ic x phys_dim x timesteps) =  (8000, 3, 121)\n",
      "dimensions of testing dataset (ic x phys_dim x timesteps) =  (2000, 3, 121)\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================================================================\n",
    "# Prepare dataset. \n",
    "# ======================================================================================================================\n",
    "# shuffle the dataset and then divide to training vs testing data sets. 80% training .20% testing.\n",
    "data_train, data_test= train_test_split(input_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"dimensions of training dataset (ic x phys_dim x timesteps) = \", np.shape(data_train))\n",
    "print(\"dimensions of testing dataset (ic x phys_dim x timesteps) = \", np.shape(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================================\n",
    "# Unit test to verify that testing and training datasets are disjoint.\n",
    "# ======================================================================================================================\n",
    "for ic_train in data_train:\n",
    "    for ic_test in data_test:\n",
    "        if ic_test[:, 0][0] == ic_train[:, 0][0] and ic_test[:, 0][1] == ic_train[:, 0][1]\\\n",
    "        and ic_test[:, 0][2] == ic_train[:, 0][2]:\n",
    "            print(\"Testing and Training datasets intersect!\")\n",
    "            print(ic_test[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datasets from numpy to tensorflow.\n",
    "data_train =  tf.data.Dataset.from_tensor_slices(data_train)\n",
    "data_test =  tf.data.Dataset.from_tensor_slices(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/200 ,loss_train: 25.878706 , loss_test: 4.0934005, run time = 359.515625 sec.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000022E0B0D05E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000022E0B0D05E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000022EA48F2C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000022EA48F2C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000022EA48F2E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000022EA48F2E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000022EA48F2F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000022EA48F2F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000022E89A661F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000022E89A661F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\encAeEx3_2020-12-02\\assets\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000022E3F01C4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000022E3F01C4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000022E89A664C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000022E89A664C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000022E89A66708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000022E89A66708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000022EA48F2DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000022EA48F2DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000022E89A66678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variable at 0x0000022E89A66678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\decAeEx3_2020-12-02\\assets\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-3ab679f92ecb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;31m# Compute gradients and then apply them to update weights within the Neural Network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmyMachine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         adam_optimizer.apply_gradients([\n\u001b[0;32m     68\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mgradients\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mrespect\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m   \"\"\"\n\u001b[1;32m--> 144\u001b[1;33m   \u001b[0mmock_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_MockOp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr_tuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m   \u001b[0mgrad_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gradient_registry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgrad_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ======================================================================================================================\n",
    "# Begin training model\n",
    "# ======================================================================================================================\n",
    "\n",
    "# initialize loss results (lists) as a function of epoch (iteration).\n",
    "train_loss_results = []\n",
    "test_loss_results = []\n",
    "\n",
    "train_dmd_loss = []\n",
    "test_dmd_loss = []\n",
    "\n",
    "train_ae_loss = []\n",
    "test_ae_loss = []\n",
    "\n",
    "train_pred_loss = []\n",
    "test_pred_loss = []\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "while epoch < (hyp_params['num_epochs']):\n",
    "    # start timer.\n",
    "    start_time = time.process_time()\n",
    "    # save the total loss of the training data and testing data.\n",
    "    epoch_loss_avg_train = tf.keras.metrics.Mean()\n",
    "    epoch_loss_avg_test = tf.keras.metrics.Mean()\n",
    "\n",
    "    # keep track of individual losses as well, aka dmd loss and ae loss.\n",
    "    epoch_loss_dmd_train = tf.keras.metrics.Mean()\n",
    "    epoch_loss_dmd_test = tf.keras.metrics.Mean()\n",
    "\n",
    "    epoch_loss_ae_train = tf.keras.metrics.Mean()\n",
    "    epoch_loss_ae_test = tf.keras.metrics.Mean()\n",
    "\n",
    "    epoch_loss_pred_train = tf.keras.metrics.Mean()\n",
    "    epoch_loss_pred_test = tf.keras.metrics.Mean()\n",
    "\n",
    "    # Build out the batches within a given epoch.\n",
    "    train_batch = data_train.shuffle(hyp_params['num_init_conds_training'], seed=42).batch(hyp_params[\"batch_size\"],\n",
    "                                                                                           drop_remainder=True)\n",
    "\n",
    "    # no need to shuffle test dataset.\n",
    "    test_batch = data_test.batch(hyp_params[\"batch_size\"], drop_remainder=True)\n",
    "\n",
    "    # Learning rate scheduling plan.  See Ch. 11 of O'Reilly.\n",
    "    if epoch % hyp_params[\"esteps\"] == 0:\n",
    "        hyp_params['lr'] = (.1 ** count) * hyp_params[\"initial_learning_rate\"]\n",
    "        adam_optimizer = tf.keras.optimizers.Adam(hyp_params['lr'])\n",
    "        count += 1\n",
    "\n",
    "    # Iterate through all the batches within an epoch.\n",
    "    for batch_training_data in train_batch:\n",
    "        # normalize batch\n",
    "\n",
    "        # Build terms that we differentiate (i.e. loss) and that we differentiate with respect to.\n",
    "        with tf.GradientTape() as tape:\n",
    "            # training=True is only needed if there are layers with different\n",
    "            # behavior during training versus inference (e.g. Dropout).\n",
    "            predictions_train = myMachine(batch_training_data)\n",
    "            ae_loss = predictions_train[3]\n",
    "            dmd_loss = predictions_train[2]\n",
    "            pred_loss = predictions_train[5]\n",
    "\n",
    "            loss_train = myLoss(batch_training_data, predictions_train)\n",
    "\n",
    "        # Compute gradients and then apply them to update weights within the Neural Network\n",
    "        gradients = tape.gradient(loss_train, myMachine.trainable_variables)\n",
    "        adam_optimizer.apply_gradients([\n",
    "            (grad, var)\n",
    "            for (grad, var) in zip(gradients, myMachine.trainable_variables)\n",
    "            if grad is not None\n",
    "        ])\n",
    "\n",
    "        # Keep track of the loss after each batch.\n",
    "        epoch_loss_avg_train.update_state(loss_train)\n",
    "        epoch_loss_ae_train.update_state(ae_loss)\n",
    "        epoch_loss_dmd_train.update_state(dmd_loss)\n",
    "        epoch_loss_pred_train.update_state(pred_loss)\n",
    "\n",
    "    for batch_test_data in test_batch:\n",
    "        predictions_test = myMachine(batch_test_data)\n",
    "        dmd_test = predictions_test[2]\n",
    "        ae_test = predictions_test[3]\n",
    "        pred_test = predictions_test[5]\n",
    "\n",
    "        loss_test = myLoss(batch_test_data, predictions_test)\n",
    "\n",
    "        epoch_loss_avg_test.update_state(loss_test)\n",
    "        epoch_loss_ae_test.update_state(ae_test)\n",
    "        epoch_loss_dmd_test.update_state(dmd_test)\n",
    "        epoch_loss_pred_test.update_state(pred_test)\n",
    "\n",
    "    train_loss_results.append(epoch_loss_avg_train.result())\n",
    "    test_loss_results.append(epoch_loss_avg_test.result())\n",
    "\n",
    "    train_dmd_loss.append(epoch_loss_dmd_train.result())\n",
    "    train_ae_loss.append(epoch_loss_ae_train.result())\n",
    "    train_pred_loss.append(epoch_loss_pred_train.result())\n",
    "\n",
    "    test_dmd_loss.append(epoch_loss_dmd_test.result())\n",
    "    test_ae_loss.append(epoch_loss_ae_test.result())\n",
    "    test_pred_loss.append(epoch_loss_pred_test.result())\n",
    "\n",
    "    if epoch % 15 == 0:\n",
    "        # save plots in results folder. Plot the latent space, ae_reconstruction, and input_batch.\n",
    "        create_plots_fluid_pred(batch_training_data, predictions_train, hyp_params, epoch, save_folder, \"train\")\n",
    "        create_plots_fluid_pred(batch_test_data, predictions_test, hyp_params, epoch, save_folder, \"test\")\n",
    "\n",
    "        # fluid latent space plots.\n",
    "        create_plots_fluid_latent(predictions_train, hyp_params, epoch,  save_folder, data_type=\"train\")\n",
    "        create_plots_fluid_latent(predictions_test, hyp_params, epoch, save_folder, data_type=\"test\")\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        # plot latent, input and reconstructed ae latest batch data.\n",
    "        print_status_bar(epoch, hyp_params[\"num_epochs\"], epoch_loss_avg_train.result(),\n",
    "                         epoch_loss_avg_test.result(), time.process_time() - start_time,\n",
    "                         log_file_path=os.path.join(\"results\", save_folder, \"log.txt\"))\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        # plot loss curves.\n",
    "        create_plots_of_loss(train_dmd_loss, train_ae_loss, test_dmd_loss, test_ae_loss, train_pred_loss,\n",
    "                             test_pred_loss, myLoss.c1, myLoss.c2, myLoss.c3, epoch, save_folder)\n",
    "\n",
    "        # save loss curves in pickle files.\n",
    "        save_loss_curves(train_loss_results, test_loss_results, train_dmd_loss, test_dmd_loss, train_ae_loss,\n",
    "                         test_ae_loss, train_pred_loss, test_pred_loss,\n",
    "                         file_path=os.path.join(\"results\", save_folder, \"Loss\"))\n",
    "\n",
    "        # save current machine.\n",
    "        myMachine.autoencoder.encoder.save(os.path.join(\"models\", str(\"enc\") + save_folder), save_format='save_weights')\n",
    "        myMachine.autoencoder.decoder.save(os.path.join(\"models\", str(\"dec\") + save_folder), save_format='save_weights')\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "# final summary of the network, again for diagnostic purposes.\n",
    "myMachine.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
