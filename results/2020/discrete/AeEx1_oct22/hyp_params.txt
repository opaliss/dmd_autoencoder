hyp_params = dict()
hyp_params['num_t_steps'] = 251
hyp_params['num_phys_dims'] = 2
hyp_params['num_init_conds'] = training_data.params['num_initial_conditions']
hyp_params['batch_size'] = 256 # MAJOR PARAMETER CHOICE
hyp_params['num_epochs'] = 200  # MAJOR PARAMETER CHOICE

# Encoding/Decoding Layer Parameters
hyp_params['num_en_layers'] = 4 # MAJOR PARAMETER CHOICE
hyp_params['num_en_neurons'] = 32  # MAJOR PARAMETER CHOICE
hyp_params['latent_dim'] = 2

hyp_params['activation'] = 'elu'
hyp_params['weight_initializer'] = 'he_uniform'
hyp_params['bias_initializer'] = 'he_uniform'
hyp_params['regfac'] = 3e-3

hyp_params['c1'] = 1 # coefficient autoencoder loss
hyp_params['c2'] = 1 # coefficient of dmd loss. 
hyp_params['c3'] = 1 # dmd reconstruction weight. 

save_folder = "AeEx1_oct22" 


# Build the AutoEncoder with affiliated loss function and optimizer.
all_data = tf.data.Dataset.from_tensor_slices(training_data.data_val)

# shuffle the dataset and then divide to training vs testing data sets. 80% training .20% testing. 
all_data_shuffle = all_data.shuffle(hyp_params['num_init_conds'], seed=42)

data_train = all_data_shuffle.take(int(0.8*hyp_params['num_init_conds']))
data_test = all_data_shuffle.skip(int(0.8*hyp_params['num_init_conds']))

hyp_params['num_init_conds_training'] = int(0.8*hyp_params['num_init_conds'])
hyp_params['num_init_conds_test'] = hyp_params['num_init_conds'] - hyp_params['num_init_conds_training']


myMachine = sm.SimpleMachine(hyp_params)
myLoss = sl.SimpleLossFunction(hyp_params)

# Learning rate initialization
lr0 = 3e-3 # MAJOR PARAMETER CHOICE
cnt = 0
estep = 50  # MAJOR PARAMETER CHOICE



tf.keras.backend.clear_session()