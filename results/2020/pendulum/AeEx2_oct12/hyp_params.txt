# Network Hyper Parameters
hyp_params = dict()
hyp_params['num_t_steps'] = 51
hyp_params['num_phys_dims'] = 2
hyp_params['num_init_conds'] = training_data.params['num_initial_conditions']
hyp_params['batch_size'] = 256 # MAJOR PARAMETER CHOICE
hyp_params['num_epochs'] = 200  # MAJOR PARAMETER CHOICE

# Encoding/Decoding Layer Parameters
hyp_params['num_en_layers'] = 2 # MAJOR PARAMETER CHOICE
hyp_params['num_en_neurons'] = 80  # MAJOR PARAMETER CHOICE
hyp_params['latent_dim'] = 2

hyp_params['activation'] = 'elu'
hyp_params['weight_initializer'] = 'he_uniform'
hyp_params['bias_initializer'] = 'he_uniform'
hyp_params['regfac'] = 3e-3

hyp_params['c1'] = 1 # coefficient autoencoder loss.
hyp_params['c2'] = 1 # coefficient of dmd loss. 
hyp_params['c3'] = 1 # coefficient of pred loss. 


save_folder = "AeEx2_oct12" # save results in the folder " Results/save_folder"- including loss curves and plot latent data. 

# Build the AutoEncoder with affiliated loss function and optimizer.
input_data = training_data.data_val/np.linalg.norm(training_data.data_val)
all_data = tf.data.Dataset.from_tensor_slices(input_data)

# shuffle the dataset and then divide to training vs testing data sets. 80% training .20% testing. 
all_data_shuffle = all_data.shuffle(hyp_params['num_init_conds'], seed=42)

data_train = all_data_shuffle.take(int(0.8*hyp_params['num_init_conds']))
data_test = all_data_shuffle.skip(int(0.8*hyp_params['num_init_conds']))

hyp_params['num_init_conds_training'] = int(0.8*hyp_params['num_init_conds'])
hyp_params['num_init_conds_test'] = hyp_params['num_init_conds'] - hyp_params['num_init_conds_training']


myMachine = sm.SimpleMachine(hyp_params)
myLoss = sl.SimpleLossFunction(hyp_params)

# Learning rate initialization
lr0 = 3e-3 # MAJOR PARAMETER CHOICE
cnt = 0
estep = 30  # MAJOR PARAMETER CHOICE


tf.keras.backend.clear_session()