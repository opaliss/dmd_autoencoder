{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train fluid flow slow manifold Machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the nonlinear mean-field model of fluid flow past a circular cylinder at Reynolds number 100, described by empirical Galerkin model:\n",
    "$$\n",
    "\\dot{x_{1}} = \\mu x_{1} - \\omega x_{2} + A x_{1}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dot{x_{2}} = \\omega x_{1} + \\mu x_{2} + Ax_{2}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dot{x_{3}} = -\\lambda(x_{3} - {x_{1}}^{2} - {x_{2}}^{2}) \n",
    "$$\n",
    "\n",
    "Where $\\mu = 0.1, \\omega = 1, A=-0.1, \\lambda = 10$\n",
    "\n",
    "Let $\\mu = \\epsilon, \\omega = 1, A=-\\epsilon, \\lambda = \\epsilon^{-1}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{1}}{\\partial t} = \\epsilon x_{1} - x_{2} - \\epsilon x_{1}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{2}}{\\partial t} =  x_{1} + \\epsilon x_{2} - \\epsilon x_{2}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{3}}{\\partial t} = -\\epsilon^{-1}(x_{3} - {x_{1}}^{2} - {x_{2}}^{2}) \n",
    "$$\n",
    "\n",
    "\n",
    "Let $\\tilde{t} = t/\\epsilon $\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{1}}{\\partial \\tilde{t}} = \\epsilon^{2} x_{1} -  \\epsilon x_{2} - \\epsilon^{2} x_{1}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{2}}{\\partial \\tilde{t}} =   \\epsilon x_{1} + \\epsilon^{2} x_{2} - \\epsilon^{2} x_{2}x_{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{3}}{\\partial \\tilde{t}} = - x_{3} + {x_{1}}^{2} + {x_{2}}^{2} \n",
    "$$\n",
    "\n",
    "\n",
    "Then \n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{1}}{\\partial \\tilde{t}}  \\approx  0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{2}}{\\partial \\tilde{t}} \\approx 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{3}}{\\partial \\tilde{t}} = - x_{3} + {x_{1}}^{2} + {x_{2}}^{2} \n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{3}(t) = \\tilde{c} e^{-t} + {x_{1}}^{2} + {x_{2}}^{2} \n",
    "$$\n",
    "\n",
    "if \n",
    "\n",
    "$$\n",
    "x_{1}(0) = r \\cos(\\theta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{2}(0) = r \\sin(\\theta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{3}(0) = \\tilde{c} e^{-0} + {r \\cos(\\theta)}^{2} + {r \\sin(\\theta)}^{2}= \\tilde{c} + r^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from dmd_machine.dmd_ae_machine import DMDMachine\n",
    "from dmd_machine.loss_function import LossFunction\n",
    "from data.Data import DataMaker\n",
    "from datetime import date \n",
    "from tensorflow.keras.models import model_from_json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from return_stats import *\n",
    "from create_plots import *\n",
    "from tensorflow import keras\n",
    "from datetime import date  \n",
    "import pickle\n",
    "import time\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3, 181)\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================================================================\n",
    "# Read in dataset.\n",
    "# ======================================================================================================================\n",
    "\n",
    "training_data = pickle.load(open('./data/dataset_fluid.pkl', 'rb'))\n",
    "\n",
    "input_data = training_data.data_val\n",
    "print(np.shape(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Hyper Parameters.\n",
    "hyp_params = dict()\n",
    "hyp_params['num_t_steps'] = training_data.params['num_time_steps']\n",
    "hyp_params['phys_dim'] = training_data.params[\"num_physical_dim\"]\n",
    "hyp_params['num_init_conds'] = training_data.params['num_initial_conditions']\n",
    "hyp_params['batch_size'] = 256\n",
    "hyp_params['num_epochs'] = 500\n",
    "\n",
    "# Encoding/Decoding Layer Parameters.\n",
    "hyp_params['num_en_layers'] = 3\n",
    "hyp_params['num_en_neurons'] = 80\n",
    "hyp_params['latent_dim'] = 2 # NEW EXPERIMENT!\n",
    "hyp_params['window_size'] = 256\n",
    "\n",
    "hyp_params['activation'] = 'elu'\n",
    "hyp_params['weight_initializer'] = 'he_uniform'\n",
    "hyp_params['bias_initializer'] = 'he_uniform'\n",
    "hyp_params['ae_output_activation'] = \"linear\"\n",
    "hyp_params['hidden_activation'] = \"elu\"\n",
    "\n",
    "hyp_params['c1'] = 1  # coefficient auto-encoder loss.\n",
    "hyp_params['c2'] = 1  # coefficient of dmd loss.\n",
    "hyp_params['c3'] = 1  # coefficient of pred loss.\n",
    "\n",
    "# save results in the folder \" Results/save_folder\"- including loss curves and plot latent data.\n",
    "save_folder = \"AeEx3_\" + str(date.today().isoformat()) \n",
    "\n",
    "# number of initial conditions in training and testing dataset.\n",
    "hyp_params['num_init_conds_training'] = int(0.8 * hyp_params['num_init_conds'])\n",
    "hyp_params['num_init_conds_test'] = hyp_params['num_init_conds'] - hyp_params['num_init_conds_training']\n",
    "\n",
    "# initialize machine and loss objects.\n",
    "myMachine = DMDMachine(hyp_params)\n",
    "# myMachine.autoencoder = keras.models.load_model(\"./models/my_model_Ex2_oct21\", compile=False)\n",
    "myLoss = LossFunction(hyp_params)\n",
    "\n",
    "# Learning rate initialization.\n",
    "hyp_params[\"initial_learning_rate\"] = 3e-3  # MAJOR PARAMETER CHOICE\n",
    "hyp_params[\"esteps\"] = 40 # MAJOR PARAMETER CHOICE\n",
    "count = 0\n",
    "\n",
    "# clear previous run session.\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# create folder to save results.\n",
    "create_new_folders(save_folder)\n",
    "\n",
    "# save hyperparams in a json file.\n",
    "save_hyp_params_in_json(hyp_params=hyp_params, json_file_path=os.path.join(\"results\", save_folder, \"hyp_params.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================================\n",
    "# Load previously trained model as the intitial weights + biases. \n",
    "# ======================================================================================================================\n",
    "load_prev = True \n",
    "\n",
    "if load_prev: \n",
    "    myMachine.autoencoder.encoder = keras.models.load_model(\"./models/encAeEx3_2021-01-19\", compile=False)\n",
    "    myMachine.autoencoder.decoder = keras.models.load_model(\"./models/decAeEx3_2021-01-19\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of training dataset (ic x phys_dim x timesteps) =  (8000, 3, 181)\n",
      "dimensions of testing dataset (ic x phys_dim x timesteps) =  (2000, 3, 181)\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================================================================\n",
    "# Prepare dataset. \n",
    "# ======================================================================================================================\n",
    "# shuffle the dataset and then divide to training vs testing data sets. 80% training .20% testing.\n",
    "data_train, data_test= train_test_split(input_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"dimensions of training dataset (ic x phys_dim x timesteps) = \", np.shape(data_train))\n",
    "print(\"dimensions of testing dataset (ic x phys_dim x timesteps) = \", np.shape(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================================\n",
    "# Unit test to verify that testing and training datasets are disjoint.\n",
    "# ======================================================================================================================\n",
    "for ic_train in data_train:\n",
    "    for ic_test in data_test:\n",
    "        if ic_test[:, 0][0] == ic_train[:, 0][0] and ic_test[:, 0][1] == ic_train[:, 0][1]\\\n",
    "        and ic_test[:, 0][2] == ic_train[:, 0][2]:\n",
    "            print(\"Testing and Training datasets intersect!\")\n",
    "            print(ic_test[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datasets from numpy to tensorflow.\n",
    "data_train =  tf.data.Dataset.from_tensor_slices(data_train)\n",
    "data_test =  tf.data.Dataset.from_tensor_slices(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/500 ,loss_train: 0.10129782 , loss_test: 0.014179544, run time = 253.28125 sec.\n",
      "INFO:tensorflow:Assets written to: models\\encAeEx3_2021-01-20\\assets\n",
      "INFO:tensorflow:Assets written to: models\\decAeEx3_2021-01-20\\assets\n",
      "10/500 ,loss_train: 0.00015217988 , loss_test: 6.0584753e-05, run time = 267.71875 sec.\n",
      "20/500 ,loss_train: 0.00011302436 , loss_test: 0.00072325894, run time = 255.234375 sec.\n",
      "30/500 ,loss_train: 0.00039246673 , loss_test: 0.00011091055, run time = 227.578125 sec.\n",
      "40/500 ,loss_train: 0.0014602453 , loss_test: 0.00016953345, run time = 711.15625 sec.\n",
      "50/500 ,loss_train: 2.3960012e-05 , loss_test: 2.401577e-05, run time = 227.359375 sec.\n",
      "INFO:tensorflow:Assets written to: models\\encAeEx3_2021-01-20\\assets\n",
      "INFO:tensorflow:Assets written to: models\\decAeEx3_2021-01-20\\assets\n",
      "60/500 ,loss_train: 2.2607195e-05 , loss_test: 2.0994721e-05, run time = 232.53125 sec.\n",
      "70/500 ,loss_train: 3.2802476e-05 , loss_test: 1.8408535e-05, run time = 224.984375 sec.\n",
      "80/500 ,loss_train: 2.740121e-05 , loss_test: 1.583276e-05, run time = 235.140625 sec.\n",
      "90/500 ,loss_train: 1.42026465e-05 , loss_test: 1.44983205e-05, run time = 246.25 sec.\n",
      "100/500 ,loss_train: 1.3507798e-05 , loss_test: 1.3852501e-05, run time = 235.671875 sec.\n",
      "INFO:tensorflow:Assets written to: models\\encAeEx3_2021-01-20\\assets\n",
      "INFO:tensorflow:Assets written to: models\\decAeEx3_2021-01-20\\assets\n",
      "110/500 ,loss_train: 1.2774364e-05 , loss_test: 1.3090897e-05, run time = 241.640625 sec.\n",
      "120/500 ,loss_train: 1.2001712e-05 , loss_test: 1.2185538e-05, run time = 447.921875 sec.\n",
      "130/500 ,loss_train: 1.1720063e-05 , loss_test: 1.1953389e-05, run time = 393.796875 sec.\n",
      "140/500 ,loss_train: 1.1483123e-05 , loss_test: 1.1714209e-05, run time = 245.296875 sec.\n",
      "150/500 ,loss_train: 1.1244676e-05 , loss_test: 1.1474835e-05, run time = 241.3125 sec.\n",
      "INFO:tensorflow:Assets written to: models\\encAeEx3_2021-01-20\\assets\n",
      "INFO:tensorflow:Assets written to: models\\decAeEx3_2021-01-20\\assets\n",
      "160/500 ,loss_train: 1.1015637e-05 , loss_test: 1.1249192e-05, run time = 246.765625 sec.\n",
      "170/500 ,loss_train: 1.0978479e-05 , loss_test: 1.1212143e-05, run time = 241.203125 sec.\n",
      "180/500 ,loss_train: 1.0938489e-05 , loss_test: 1.1172079e-05, run time = 296.78125 sec.\n",
      "190/500 ,loss_train: 1.0900425e-05 , loss_test: 1.1132548e-05, run time = 265.71875 sec.\n",
      "200/500 ,loss_train: 1.0860805e-05 , loss_test: 1.1097065e-05, run time = 258.265625 sec.\n",
      "INFO:tensorflow:Assets written to: models\\encAeEx3_2021-01-20\\assets\n",
      "INFO:tensorflow:Assets written to: models\\decAeEx3_2021-01-20\\assets\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-b9b87f568327>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[1;31m# training=True is only needed if there are layers with different\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;31m# behavior during training versus inference (e.g. Dropout).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mpredictions_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyMachine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_training_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m             \u001b[0mae_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdmd_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\dmd_autoencoder\\dmd_machine\\dmd_ae_machine.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_predict_batch_reshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_data_mat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# decode predicted latent space.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\dmd_autoencoder\\dmd_machine\\dmd_ae_machine.py\u001b[0m in \u001b[0;36mcompute_predict_batch_reshape\u001b[1;34m(self, y_data_mat)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;31m# find the big A mat (512 x 512)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mpred_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_predicted_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_reshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;31m# undo reshape- back to batch_size, phys_dim, t_steps.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\dmd_autoencoder\\dmd_machine\\dmd_ae_machine.py\u001b[0m in \u001b[0;36mget_predicted_y\u001b[1;34m(self, y_data)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_t_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[0mA_exp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_exp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m             \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mii\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_exp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   3313\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3314\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[1;32m-> 3315\u001b[1;33m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[0;32m   3316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   5526\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m   5527\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_b\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5528\u001b[1;33m         transpose_b)\n\u001b[0m\u001b[0;32m   5529\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ======================================================================================================================\n",
    "# Begin training model\n",
    "# ======================================================================================================================\n",
    "\n",
    "# initialize loss results (lists) as a function of epoch (iteration).\n",
    "train_loss_results = []\n",
    "test_loss_results = []\n",
    "\n",
    "train_dmd_loss = []\n",
    "test_dmd_loss = []\n",
    "\n",
    "train_ae_loss = []\n",
    "test_ae_loss = []\n",
    "\n",
    "train_pred_loss = []\n",
    "test_pred_loss = []\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "while epoch < (hyp_params['num_epochs']):\n",
    "    # start timer.\n",
    "    start_time = time.process_time()\n",
    "    # save the total loss of the training data and testing data.\n",
    "    epoch_loss_avg_train = tf.keras.metrics.Mean()\n",
    "    epoch_loss_avg_test = tf.keras.metrics.Mean()\n",
    "\n",
    "    # keep track of individual losses as well, aka dmd loss and ae loss.\n",
    "    epoch_loss_dmd_train = tf.keras.metrics.Mean()\n",
    "    epoch_loss_dmd_test = tf.keras.metrics.Mean()\n",
    "\n",
    "    epoch_loss_ae_train = tf.keras.metrics.Mean()\n",
    "    epoch_loss_ae_test = tf.keras.metrics.Mean()\n",
    "\n",
    "    epoch_loss_pred_train = tf.keras.metrics.Mean()\n",
    "    epoch_loss_pred_test = tf.keras.metrics.Mean()\n",
    "\n",
    "    # Build out the batches within a given epoch.\n",
    "    train_batch = data_train.shuffle(hyp_params['num_init_conds_training'], seed=42).batch(hyp_params[\"batch_size\"],\n",
    "                                                                                           drop_remainder=True)\n",
    "\n",
    "    # no need to shuffle test dataset.\n",
    "    test_batch = data_test.batch(hyp_params[\"batch_size\"], drop_remainder=True)\n",
    "\n",
    "    # Learning rate scheduling plan.  See Ch. 11 of O'Reilly.\n",
    "    if epoch % hyp_params[\"esteps\"] == 0:\n",
    "        hyp_params['lr'] = (.1 ** count) * hyp_params[\"initial_learning_rate\"]\n",
    "        adam_optimizer = tf.keras.optimizers.Adam(hyp_params['lr'])\n",
    "        count += 1\n",
    "\n",
    "    # Iterate through all the batches within an epoch.\n",
    "    for batch_training_data in train_batch:\n",
    "        # normalize batch\n",
    "\n",
    "        # Build terms that we differentiate (i.e. loss) and that we differentiate with respect to.\n",
    "        with tf.GradientTape() as tape:\n",
    "            # training=True is only needed if there are layers with different\n",
    "            # behavior during training versus inference (e.g. Dropout).\n",
    "            predictions_train = myMachine(batch_training_data)\n",
    "            ae_loss = predictions_train[3]\n",
    "            dmd_loss = predictions_train[2]\n",
    "            pred_loss = predictions_train[5]\n",
    "\n",
    "            loss_train = myLoss(batch_training_data, predictions_train)\n",
    "\n",
    "        # Compute gradients and then apply them to update weights within the Neural Network\n",
    "        gradients = tape.gradient(loss_train, myMachine.trainable_variables)\n",
    "        adam_optimizer.apply_gradients([\n",
    "            (grad, var)\n",
    "            for (grad, var) in zip(gradients, myMachine.trainable_variables)\n",
    "            if grad is not None\n",
    "        ])\n",
    "\n",
    "        # Keep track of the loss after each batch.\n",
    "        epoch_loss_avg_train.update_state(loss_train)\n",
    "        epoch_loss_ae_train.update_state(ae_loss)\n",
    "        epoch_loss_dmd_train.update_state(dmd_loss)\n",
    "        epoch_loss_pred_train.update_state(pred_loss)\n",
    "\n",
    "    for batch_test_data in test_batch:\n",
    "        predictions_test = myMachine(batch_test_data)\n",
    "        dmd_test = predictions_test[2]\n",
    "        ae_test = predictions_test[3]\n",
    "        pred_test = predictions_test[5]\n",
    "\n",
    "        loss_test = myLoss(batch_test_data, predictions_test)\n",
    "\n",
    "        epoch_loss_avg_test.update_state(loss_test)\n",
    "        epoch_loss_ae_test.update_state(ae_test)\n",
    "        epoch_loss_dmd_test.update_state(dmd_test)\n",
    "        epoch_loss_pred_test.update_state(pred_test)\n",
    "\n",
    "    train_loss_results.append(epoch_loss_avg_train.result())\n",
    "    test_loss_results.append(epoch_loss_avg_test.result())\n",
    "\n",
    "    train_dmd_loss.append(epoch_loss_dmd_train.result())\n",
    "    train_ae_loss.append(epoch_loss_ae_train.result())\n",
    "    train_pred_loss.append(epoch_loss_pred_train.result())\n",
    "\n",
    "    test_dmd_loss.append(epoch_loss_dmd_test.result())\n",
    "    test_ae_loss.append(epoch_loss_ae_test.result())\n",
    "    test_pred_loss.append(epoch_loss_pred_test.result())\n",
    "\n",
    "    if epoch % 15 == 0:\n",
    "        # save plots in results folder. Plot the latent space, ae_reconstruction, and input_batch.\n",
    "        create_plots_fluid_pred(batch_training_data, predictions_train, hyp_params, epoch, save_folder, \"train\")\n",
    "        create_plots_fluid_pred(batch_test_data, predictions_test, hyp_params, epoch, save_folder, \"test\")\n",
    "        \n",
    "        # if latent dim is 3 dimensional then create 3d plots. \n",
    "        if hyp_params[\"latent_dim\"] == 3: \n",
    "            # fluid latent space plots.\n",
    "            create_plots_fluid_latent_3d(predictions_train, hyp_params, epoch,  save_folder, data_type=\"train\")\n",
    "            create_plots_fluid_latent_3d(predictions_test, hyp_params, epoch, save_folder, data_type=\"test\")\n",
    "        \n",
    "        # if latent fim is 2 dimensional, then create 2d plots. \n",
    "        elif hyp_params[\"latent_dim\"] == 2: \n",
    "            create_plots_fluid_latent_2d(predictions_train, hyp_params, epoch,  save_folder, data_type=\"train\")\n",
    "            create_plots_fluid_latent_2d(predictions_test, hyp_params, epoch, save_folder, data_type=\"test\")\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        # plot latent, input and reconstructed ae latest batch data.\n",
    "        print_status_bar(epoch, hyp_params[\"num_epochs\"], epoch_loss_avg_train.result(),\n",
    "                         epoch_loss_avg_test.result(), time.process_time() - start_time,\n",
    "                         log_file_path=os.path.join(\"results\", save_folder, \"log.txt\"))\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        # plot loss curves.\n",
    "        create_plots_of_loss(train_dmd_loss, train_ae_loss, test_dmd_loss, test_ae_loss, train_pred_loss,\n",
    "                             test_pred_loss, myLoss.c1, myLoss.c2, myLoss.c3, epoch, save_folder)\n",
    "\n",
    "        # save loss curves in pickle files.\n",
    "        save_loss_curves(train_loss_results, test_loss_results, train_dmd_loss, test_dmd_loss, train_ae_loss,\n",
    "                         test_ae_loss, train_pred_loss, test_pred_loss,\n",
    "                         file_path=os.path.join(\"results\", save_folder, \"Loss\"))\n",
    "\n",
    "        # save current machine.\n",
    "        myMachine.autoencoder.encoder.save(os.path.join(\"models\", str(\"enc\") + save_folder), save_format='save_weights')\n",
    "        myMachine.autoencoder.decoder.save(os.path.join(\"models\", str(\"dec\") + save_folder), save_format='save_weights')\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "# final summary of the network, again for diagnostic purposes.\n",
    "myMachine.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
